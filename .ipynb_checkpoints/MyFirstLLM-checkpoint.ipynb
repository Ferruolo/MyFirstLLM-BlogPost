{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10912c7a-c6c8-4bfc-a96f-e6754cecfc64",
   "metadata": {},
   "source": [
    "# My first LLM finetune\n",
    "### A blog post by Andrew Ferruolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ab5958-3582-4b10-a060-37f37d6c131b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## My project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8fcee-ec90-4610-a5a5-4da4c48d1f89",
   "metadata": {},
   "source": [
    "If you're like me, you have to do most of your writing in C++ or Python, not in English. While I'm grateful that I get to code every day, this does lead to me making regular grammar and spelling issues. I often look at my writing and wish I could figure out how to word it better, or more naturally. I initially applied ChatGPT and Claude to this probelem, but found that they would often change my style, and even sometimes the meaning of my sentences. This is because ChatGPT and Claude are trained and managed behind closed doors, by companies who train the models to enforce their opinions on users in the name of AI saftey. While I understand that these companies need to provide some form of control and bias to prevent bad outcomes from usage of their products, I dislike having my writing altered to agree with other people's opinion. And so, I decided what I needed was to finetune an open source LLM for my purposes, and find a way to run it on my local computer. Here is a documented account of my journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208c992-5731-417e-8bc4-7b78c65c23a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirments and Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e724a0b-9ce1-4b05-942d-c69995d7d643",
   "metadata": {},
   "source": [
    "From the above paragraph, we can elicit the following requirements for my project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd71324-b5b0-486a-9e3e-0f9e55c4455d",
   "metadata": {},
   "source": [
    "1. Simple - I was a Michgan CS student at the time of the project. I didn't exactly have days to throw at this project\n",
    "2. Open Source - The goal is to get a strong understanding of LLMs, tools, frameworks, etc. \n",
    "3. Fast - If it takes 30 minutes for the LLM to run, I might as well just do the work myself.\n",
    "4. Memory Efficient - LLMs are large by nature, but my Mac only has so much RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74f765-1ee9-42e0-8eb9-6fc0f3d6ea89",
   "metadata": {},
   "source": [
    "Given these requirments, I landed on the following implementation specifications:\n",
    "    \n",
    "1. Use Huggingface to train, use basic adaptation of Llama.cpp to deploy\n",
    "2. Use Llama2 7B and finetune - Easily works with Llama.cpp, and I'm a huge fan of Yann LeCun and FAIR.\n",
    "3. Weights must be quantized. Possibly, we might want to prune for a more performant and smaller network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c06bf8-4e08-4308-9fb6-c8265b2cabac",
   "metadata": {},
   "source": [
    "## We have our specs. Now, let's start building it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6369b2b-b646-490c-b407-63ed87aecbcd",
   "metadata": {},
   "source": [
    "To finetune a large language model, we need some powerful servers and GPUS. Training 7B params, even quantized, on my local computer would likely lead to it bursting into flames. So, I had to use the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78e73c-9055-4a79-a01a-d3dcae9baf8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### My first instinct: use AWS (an expensive and unfruitful journey)\n",
    "\n",
    "This, obviously, was a mistake. I don't know if you have used SageMaker before, but within a day my \"Cost and usage\" tab looked a little something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393cb3e-1efe-4fb0-ad40-c179715d8b3a",
   "metadata": {},
   "source": [
    "![AWS Cost](aws-csot.gif \"AWS COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0358dbc-5aa5-42c3-8777-ef5ab79d2f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Well. I guess it's worth it. I'm paying for an easy to use, intuitive, flexible interface right? WRONG:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e9300-a463-43c4-8bfa-b1958dbc0017",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"SageMaker-Confusing.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb804-9856-450e-8bae-fb629fbdab72",
   "metadata": {},
   "source": [
    "What even is this! I just want a basic instance I can SSH into. Even deleteing profiles, instances, and the rest from AWS is a huge pain. Although I'm sure that at the enterprise level there is a good reason for all of this, I personally don't want to deal with it. So, I'm just not going to. Bye Jeff!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efcf75-d796-4b62-a031-5412d397311a",
   "metadata": {},
   "source": [
    "### A Better Solution: Brev.dev\n",
    "\n",
    "I then remembered a company called Brev.dev, which I had seen on Twitter a couple months ago. After checking them out, I discovered they agreed with me on how AWS is nearly unuseable, and created a solution for AI hackers like me. Their simple interface allowed me to complete the rest of my project in just a few hours, and for minimal cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84245e7e-e3f9-4e23-be0a-438fa299b8e1",
   "metadata": {},
   "source": [
    "### Spinning up is simple\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"Brev-Spinup.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Managing is just as simple\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-start.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### SSH is easier than ever before\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-ssh.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c7f62-e5a9-41e5-b2a5-067522f1e9fa",
   "metadata": {},
   "source": [
    "### Now that's what we like to see\n",
    "Brev provides a simple, clean interface to get new GPUS. Also, they're insanely cheap. Look at those prices! Finetuning might cost me less than a burger if I do it right! Now that we have our instance all set up, lets start actually doing work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139968e-648c-41de-9188-4a7e9fc543cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting my dataset/model ready for huggingface:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2450-9515-4bde-add7-6a99962c8740",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Create directory called \"grammar_dataset, with two subdirectories called \"train\" and \"validation\"\n",
    "2. Download data using \"download_grammar_dataset.py\"\n",
    "3. Move \"gtrain_10k.csv\" to train, \"grammar_validation.csv\" to validation\n",
    "4. Add a readme to grammar dataset, with following as follows: (copy directly from cell below)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5534bfa-3a87-4161-98da-b9900887d4b5",
   "metadata": {},
   "source": [
    "---\n",
    "configs:\n",
    "- config_name: default\n",
    "  data_files:\n",
    "  - split: train\n",
    "    path: \"train/gtrain_10k.csv\"\n",
    "  - split: validation\n",
    "    path: \"validation/grammar_validation.csv\"\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98659e3c-6099-4d07-9bea-eb809e5f530e",
   "metadata": {},
   "source": [
    "Now we need to get our model. \n",
    "5. Visit this link (https://llama.meta.com/llama-downloads/), and follow the instructions to download your desired model to your instance\n",
    "6. Use the \"convert_llama_weights_to_hf.py\" script in this repo to convert your weights to huggingface format\n",
    "7. If your filetree looks like like the one below, your're ready to go!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8cc36d-e5e4-426e-b2ac-962ef789c419",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Filetree here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c487d7-9e93-40b4-9c68-ee7ec654bc46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning my model, using huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5a5e8-5a9e-4e08-beb2-532e50791f5e",
   "metadata": {},
   "source": [
    "Warning: This part might get a little dense. You'll have to forgive me if I breeze over some explanations. For a deeper explanation, take a look at Harper Carroll's blog here: (https://brev.dev/blog/how-qlora-works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "534ea79f-bf63-42f9-b536-9c32d7f944ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# First, we import\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1e6bd6d7-d025-4f7c-b8a4-d2b8fc97025c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "llama_og_path = \"./models/llama-7b-huggingface\"\n",
    "llama_token_path = \"./models/llama-7b-huggingface\"\n",
    "dataset = \"./llama_datasets/grammar_dataset/\" #gtrain_10k.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d48964-3a0b-4ae4-911a-37b5ac5e6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(train_dataset, split='train')\n",
    "eval_dataset  = load_dataset(test_dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98cdc1c9-94ad-47a6-b305-3c0cef124cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'AutoTokenizer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m      2\u001b[0m     llama_token_path,\n\u001b[1;32m      3\u001b[0m     model_max_length\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m,\n\u001b[1;32m      4\u001b[0m     padding_side\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m      5\u001b[0m     add_eos_token\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      7\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mpad_token \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbos_token\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtokenize\u001b[39m(prompt):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AutoTokenizer' is not defined"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    model_max_length=256,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d553a4f-25a3-4174-8bc7-8e3334bf3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    target = data_point['input']\n",
    "    result = data_point['target']\n",
    "    \n",
    "    full_prompt = f\"You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, \\\n",
    "        the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {target} CORRECT: {result}\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f17af-68d0-4e42-b7d8-884a02b1e62f",
   "metadata": {},
   "source": [
    "### Prep Model For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac77b7b2-26a0-48fe-ab22-58a5a0502d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e31191e-2d31-4985-9752-6ba4fb9d8467",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(llama_og_path, quantization_config=bnb_config)\n",
    "model = PeftModel.from_pretrained(base_model, \"./llama2-grammar/checkpoint-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356a7ca4-a176-41cf-82b3-95e1157da041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_prompt = \"It's great to be  \"\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    padding_side=\"left\",\n",
    "    model_max_length=256,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=256)[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4b4610-aa17-4e02-baaa-873b12d0d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a47d61f1-30ed-458d-be52-b1be02381341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fc324fc-5e36-4e39-a58a-5873f5888e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "model = accelerator.prepare_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d48f241-d051-4c42-9fbe-e484d57a28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f353b5-87df-4a26-b164-607395e7db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"grammar\"\n",
    "base_model_name = \"llama2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",   \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every 50 logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b0a87-1391-48ee-88f8-0726067729a8",
   "metadata": {},
   "source": [
    "# We Trained, now what?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5b3a0-a1f4-4214-aa1a-1783a6787656",
   "metadata": {},
   "source": [
    "## First lets test the model\n",
    "1. Hit ESC-00 (reset the kernel), then evaluate by hand using the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a719f1a2-1270-4cd3-a6fd-5c0470d96b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'accelerate'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01maccelerate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullyShardedDataParallelPlugin, Accelerator\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfsdp\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfully_sharded_data_parallel\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FullOptimStateDictConfig, FullStateDictConfig\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'accelerate'"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3b3839-7707-401e-8046-f303101c79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"\" # TODO: Put Checkpoint path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27162a17-14b1-45d8-9c2a-d000639b993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = load_dataset(test_dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "829a146a-73fb-4c36-81d7-c9c345b88eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5150a5f6-4961-49d2-949f-734c4db47fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_og_path,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c643e61-ff6a-47f8-bd8e-2c5f90ae3848",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bos' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 8\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Change i to change the input prompt. Evaluate the\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# outputs using your own judgement. Adjust hyperparameters above for training \u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# or prompt to improve your response\u001b[39;00m\n\u001b[1;32m      5\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m50\u001b[39m\n\u001b[0;32m----> 8\u001b[0m eval_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbos\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124mYou will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues,\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \\\n\u001b[1;32m      9\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meval_dataset[i][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m CORRECT: \u001b[39m\u001b[38;5;124m\"\u001b[39m \n\u001b[1;32m     12\u001b[0m model_input \u001b[38;5;241m=\u001b[39m tokenizer(eval_prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m base_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bos' is not defined"
     ]
    }
   ],
   "source": [
    "# Change i to change the input prompt. Evaluate the\n",
    "# outputs using your own judgement. Adjust hyperparameters above for training \n",
    "# or prompt to improve your response\n",
    "\n",
    "i = 50\n",
    "\n",
    "\n",
    "eval_prompt = f\"{bos}You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues,\" + \\\n",
    "        f\" the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {eval_dataset[i]['input']} CORRECT: \" \n",
    "\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\")\n",
    "\n",
    "base_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = tokenizer.decode(base_model.generate(**model_input, max_new_tokens=150, repetition_penalty=1.15)[0], skip_special_tokens=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7b6a0-8264-4da7-881d-0271cbcbe04a",
   "metadata": {},
   "source": [
    "# Now we have a good model. \n",
    "We have to merge our LORA's and dequantize it before we can export to Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8d1619da-c3e8-4275-92fc-bb4f33ae7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish me"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db3efe-04c0-4d22-a5a1-e30a4495ab62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now that we have merged, we can export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8057a-7a25-4cc9-8937-799086b16a03",
   "metadata": {},
   "source": [
    "We download our weights by taring them up and using SCP\n",
    "```console\n",
    "tar -cvf models/model-final final-model.tar.gz\n",
    "```\n",
    "\n",
    "Now on your local instance, in a clone of Llama.cpp (Use my modifications here, under \"examples/llamacheck\" (https://github.com/Ferruolo/llama.cpp). Make using\n",
    "make llamacheck\n",
    "```\n",
    "scp -i myBrevInstanceName:myFirstLLM/final-model.tar.gz ./models\n",
    "tar -xvf ./models/final-model.tar.gz\n",
    "python convert-hf-to-gguf.py ./models/final-model --outfile llamacheck.gguf\n",
    "TODO: Quantize llamacheck.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd5b4e-4468-42cb-b54d-e04b8b37796a",
   "metadata": {},
   "source": [
    "# We're done. A brief reflection\n",
    "\n",
    "Being honest, my experience was not as simple as this blog post. Even though I adapted most of my code directly from the notebooks I actually used, I took a lot of twists and turns, misinterpreted docs several times, and struggled to export my weights. I hope that by compiling everything I did into one big notebook, I have made your life significantly easier! I was suprised by how easy finetuning was once I had figured everything out, but was dismayed at how difficult doing practical things with huggingface, bits and bytes library (QLORA), and llama.cpp were. I found myself in a configuration hell. There's no reason that exporting to another format should be as complicated as it was. \n",
    "\n",
    "\n",
    "I would like to thank the Brev team for helping me learn a lot of the things I talk about here, and giving me the opportunity to write this blog for them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
