{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10912c7a-c6c8-4bfc-a96f-e6754cecfc64",
   "metadata": {},
   "source": [
    "# My First LLM Finetune\n",
    "### A blog post by Andrew Ferruolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbbb7a-8f17-46ae-bad4-ec52eac86e66",
   "metadata": {},
   "source": [
    "## Some Background Knowledge\n",
    "I understand not everyone reading this is going to know all the terms I throw around here. So, here is a basic dictionary to help you out\n",
    "\n",
    "Large Langauge Model (LLM) - A large AI model trained for interacting with text. ChatGPT and Claude are examples of LLMs \\\n",
    "HuggingFace - Huggingface transformers library is a package which allows us to work with LLMs while taking away a lot of the complexity. \\\n",
    "Package - Prewritten code downloaded off the internet \\\n",
    "Train/Finetune - To train an AI model means running the model over a dataset and having it make predictions, and then correcting the model parameters by assessing the correctness of its predictions. Finetuning is just continued training\\\n",
    "Llama - A LLM developer by Facebook AI research \\\n",
    "Llama.cpp - A deployment framework for llama written in C++ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8fcee-ec90-4610-a5a5-4da4c48d1f89",
   "metadata": {},
   "source": [
    "## My project\n",
    "\n",
    "If you're like me, you have to do most of your writing in C++ or Python, not in English. While I'm grateful that I get to code every day, this does lead to me making regular grammar and spelling issues. I often look at my writing and wish I could figure out how to word it better, or more naturally. I initially applied ChatGPT and Claude to this probelem, but found that they would often change my style, and even sometimes the meaning of my sentences. This is because ChatGPT and Claude are trained and managed behind closed doors, by companies who train the models to enforce their opinions on users in the name of AI saftey. While I understand that these companies need to provide some form of control and bias to prevent bad outcomes from usage of their products, I dislike having my writing altered to agree with other people's opinion. And so, I decided what I needed was to finetune an open source Large Language Model (LLM) for my purposes, and find a way to run it on my local computer. Here is a documented account of my journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208c992-5731-417e-8bc4-7b78c65c23a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirments and Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e724a0b-9ce1-4b05-942d-c69995d7d643",
   "metadata": {},
   "source": [
    "From the above paragraph, we can elicit the following requirements for my project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd71324-b5b0-486a-9e3e-0f9e55c4455d",
   "metadata": {},
   "source": [
    "1. Simple - I was a Michgan CS student at the time of the project. I didn't exactly have days to throw at this project\n",
    "2. Open Source - The goal is to get a strong understanding of LLMs, tools, frameworks, etc. \n",
    "3. Fast - If it takes 30 minutes for the LLM to run, I might as well just do the work myself.\n",
    "4. Memory Efficient - LLMs are large by nature, but my Mac only has so much RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74f765-1ee9-42e0-8eb9-6fc0f3d6ea89",
   "metadata": {},
   "source": [
    "Given these requirments, I landed on the following implementation specifications:\n",
    "    \n",
    "1. Use Huggingface to train, use basic adaptation of Llama.cpp to deploy\n",
    "2. Use Llama2 7B and finetune - Easily works with Llama.cpp, and I'm a huge fan of Yann LeCun and FAIR.\n",
    "3. Weights must be quantized. Possibly, we might want to prune for a more performant and smaller network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c06bf8-4e08-4308-9fb6-c8265b2cabac",
   "metadata": {},
   "source": [
    "## We have our specs. Now, let's start building it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6369b2b-b646-490c-b407-63ed87aecbcd",
   "metadata": {},
   "source": [
    "To finetune a large language model, we need some powerful servers and GPUS. Training 7B params, even quantized, on my local computer would likely lead to it bursting into flames. So, I had to use the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78e73c-9055-4a79-a01a-d3dcae9baf8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### My first instinct: use AWS (an expensive and unfruitful journey)\n",
    "\n",
    "This, obviously, was a mistake. I don't know if you have used SageMaker before, but within a day my \"Cost and usage\" tab looked a little something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393cb3e-1efe-4fb0-ad40-c179715d8b3a",
   "metadata": {},
   "source": [
    "![AWS Cost](aws-csot.gif \"AWS COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0358dbc-5aa5-42c3-8777-ef5ab79d2f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Well. I guess it's worth it. I'm paying for an easy to use, intuitive, flexible interface right? WRONG:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e9300-a463-43c4-8bfa-b1958dbc0017",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"SageMaker-Confusing.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb804-9856-450e-8bae-fb629fbdab72",
   "metadata": {},
   "source": [
    "What even is this! I just want a basic interface. Even deleteing profiles, instances, and the rest from AWS is a huge pain. Although I'm sure that at the enterprise level there is a good reason for all of this, I personally don't want to deal with it. So, I'm just not going to. Bye Jeff!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efcf75-d796-4b62-a031-5412d397311a",
   "metadata": {},
   "source": [
    "### A Better Solution: Brev.dev\n",
    "\n",
    "I then remembered a company called Brev.dev, which I had seen on Twitter a couple months ago. After checking them out, I discovered they agreed with me on how AWS is nearly unuseable, and created a solution for AI hackers like me. Their simple interface allowed me to complete the rest of my project in just a few hours, and for minimal cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84245e7e-e3f9-4e23-be0a-438fa299b8e1",
   "metadata": {},
   "source": [
    "### Spinning up is simple\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"Brev-Spinup.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Managing is just as simple\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-start.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### SSH is easier than ever before\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-ssh.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c7f62-e5a9-41e5-b2a5-067522f1e9fa",
   "metadata": {},
   "source": [
    "### Now that's what we like to see\n",
    "Brev provides a simple, clean interface to get new GPUS. Also, they're insanely cheap. Look at those prices! Finetuning might cost me less than a burger if I do it right! Now that we have our instance all set up, lets start actually doing work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139968e-648c-41de-9188-4a7e9fc543cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting my dataset/model ready for huggingface:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2450-9515-4bde-add7-6a99962c8740",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Create directory called \"grammar_dataset, with two subdirectories called \"train\" and \"validation\"\n",
    "2. Download data using \"download_grammar_dataset.py\"\n",
    "3. Move \"gtrain_10k.csv\" to train, \"grammar_validation.csv\" to validation\n",
    "4. Add a readme to grammar dataset, with following as follows: (copy directly from cell below, between the two quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5534bfa-3a87-4161-98da-b9900887d4b5",
   "metadata": {},
   "source": [
    "```console\n",
    "---\n",
    "configs:\n",
    "- config_name: default\n",
    "  data_files:\n",
    "  - split: train\n",
    "    path: \"train/gtrain_10k.csv\"\n",
    "  - split: validation\n",
    "    path: \"validation/grammar_validation.csv\"\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98659e3c-6099-4d07-9bea-eb809e5f530e",
   "metadata": {},
   "source": [
    "Now we need to get our model\n",
    "\n",
    "5. Visit this link (https://llama.meta.com/llama-downloads/), and follow the instructions to download your desired model to your instance\n",
    "6. Use the \"convert_llama_weights_to_hf.py\" script in this repo to convert your weights to huggingface format\n",
    "7. If your filetree looks like like the one below, your're ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5697c9-a1df-4448-9473-a43e2a647fca",
   "metadata": {},
   "source": [
    "```console\n",
    ".\n",
    "├── MyFirstLLM.ipynb\n",
    "├── datasets\n",
    "│   └── grammar_dataset\n",
    "│       ├── README.md\n",
    "│       ├── train\n",
    "│       │   └── gtrain_10k.csv\n",
    "│       └── validation\n",
    "│           └── grammar_validation.csv\n",
    "├── models\n",
    "│   ├── convert_llama_weights_to_hf.py\n",
    "│   ├── llama-7B-huggingface\n",
    "│   │   ├── config.json\n",
    "│   │   ├── generation_config.json\n",
    "│   │   ├── pytorch_model-00001-of-00003.bin\n",
    "│   │   ├── pytorch_model-00002-of-00003.bin\n",
    "│   │   ├── pytorch_model-00003-of-00003.bin\n",
    "│   │   ├── pytorch_model.bin.index.json\n",
    "│   │   ├── special_tokens_map.json\n",
    "│   │   ├── tokenizer.json\n",
    "│   │   ├── tokenizer.model\n",
    "│   │   └── tokenizer_config.json\n",
    "│   └── llama-7B-pytorch\n",
    "│       ├── checklist.chk\n",
    "│       ├── consolidated.00.pth\n",
    "│       ├── params.json\n",
    "│       └── tokenizer.model\n",
    "└── readme.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c487d7-9e93-40b4-9c68-ee7ec654bc46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning my model, using huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5a5e8-5a9e-4e08-beb2-532e50791f5e",
   "metadata": {},
   "source": [
    "Warning: This part might get a little dense. You'll have to forgive me if I breeze over some explanations. For a deeper explanation, take a look at Harper Carroll's blog here: (https://brev.dev/blog/how-qlora-works)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a743bc1-10b5-48f4-ad34-95b41d2d4fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (2.0.1)\n",
      "Requirement already satisfied: filelock in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from jinja2->torch) (2.1.1)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from sympy->torch) (1.3.0)\n",
      "Requirement already satisfied: transformers in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (4.29.2)\n",
      "Requirement already satisfied: filelock in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (0.23.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (2022.7.9)\n",
      "Requirement already satisfied: requests in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (2.31.0)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (0.13.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers) (4.65.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2024.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->transformers) (2024.2.2)\n",
      "ERROR: unknown command \"instal\" - maybe you meant \"install\"\n",
      "Requirement already satisfied: peft in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (0.11.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (23.0)\n",
      "Requirement already satisfied: psutil in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: pyyaml in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (6.0)\n",
      "Requirement already satisfied: torch>=1.13.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (2.0.1)\n",
      "Requirement already satisfied: transformers in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (4.29.2)\n",
      "Requirement already satisfied: tqdm in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (4.65.0)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (0.30.1)\n",
      "Requirement already satisfied: safetensors in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (0.4.3)\n",
      "Requirement already satisfied: huggingface-hub>=0.17.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from peft) (0.23.1)\n",
      "Requirement already satisfied: filelock in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2024.5.0)\n",
      "Requirement already satisfied: requests in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub>=0.17.0->peft) (4.7.1)\n",
      "Requirement already satisfied: sympy in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (1.11.1)\n",
      "Requirement already satisfied: networkx in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1)\n",
      "Requirement already satisfied: jinja2 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from torch>=1.13.0->peft) (3.1.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers->peft) (2022.7.9)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from transformers->peft) (0.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n",
      "Requirement already satisfied: datasets in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (2.12.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (1.24.3)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (11.0.0)\n",
      "Requirement already satisfied: dill<0.3.7,>=0.3.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (0.3.6)\n",
      "Requirement already satisfied: pandas in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (1.5.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (4.65.0)\n",
      "Requirement already satisfied: xxhash in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (2.0.2)\n",
      "Requirement already satisfied: multiprocess in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (0.70.14)\n",
      "Requirement already satisfied: fsspec>=2021.11.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from fsspec[http]>=2021.11.1->datasets) (2024.5.0)\n",
      "Requirement already satisfied: aiohttp in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (3.8.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0.0,>=0.11.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (0.23.1)\n",
      "Requirement already satisfied: packaging in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (23.0)\n",
      "Requirement already satisfied: responses<0.19 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (0.13.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from datasets) (6.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (22.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (2.0.4)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (6.0.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.8.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: filelock in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.7.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: six in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from responses<0.19->datasets) (1.16.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/andrewferruolo/anaconda3/lib/python3.11/site-packages (from pandas->datasets) (2022.7)\n"
     ]
    }
   ],
   "source": [
    "#Install Eveything\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip instal bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534ea79f-bf63-42f9-b536-9c32d7f944ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e6bd6d7-d025-4f7c-b8a4-d2b8fc97025c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "llama_og_path = \"./models/llama-7B-huggingface\"\n",
    "llama_token_path = \"./models/llama-7B-huggingface\"\n",
    "dataset = \"./datasets/grammar_dataset/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d48964-3a0b-4ae4-911a-37b5ac5e6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(dataset, split='train')\n",
    "eval_dataset  = load_dataset(dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cdc1c9-94ad-47a6-b305-3c0cef124cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    model_max_length=256,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d553a4f-25a3-4174-8bc7-8e3334bf3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    target = data_point['input']\n",
    "    result = data_point['target']\n",
    "    \n",
    "    full_prompt = f\"You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, \\\n",
    "        the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {target} CORRECT: {result}\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f17af-68d0-4e42-b7d8-884a02b1e62f",
   "metadata": {},
   "source": [
    "### Prep Model For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac77b7b2-26a0-48fe-ab22-58a5a0502d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c34e8ab-7118-47dc-82e1-191213a2c6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494a2129c83c4f0b94b39850ac73a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_og_path, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356a7ca4-a176-41cf-82b3-95e1157da041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The University of Michgian 2018-19 Men's Basketball News\n",
      " nobody can stop the Wolverines\n"
     ]
    }
   ],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_prompt = \"The University of Michgian \" # GO BLUE!\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    padding_side=\"left\",\n",
    "    model_max_length=20,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=20)[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4b4610-aa17-4e02-baaa-873b12d0d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a47d61f1-30ed-458d-be52-b1be02381341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc324fc-5e36-4e39-a58a-5873f5888e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15207936 || all params: 3515620864 || trainable%: 0.4325817995828119\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# model = acelerator.prepare_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d48f241-d051-4c42-9fbe-e484d57a28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f353b5-87df-4a26-b164-607395e7db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"grammar\"\n",
    "base_model_name = \"llama2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",   \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every 50 logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705f5f3-4521-4ae5-b6fb-b3000068c395",
   "metadata": {},
   "source": [
    "You'll get an output that looks like this: (training SS).\n",
    "\n",
    "\n",
    "\n",
    "Run until your Validation Loss is no longer decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b0a87-1391-48ee-88f8-0726067729a8",
   "metadata": {},
   "source": [
    "# We Trained, Now What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5b3a0-a1f4-4214-aa1a-1783a6787656",
   "metadata": {},
   "source": [
    "## First lets test the model\n",
    "1. Hit ESC-00 (reset the kernel), then evaluate by hand using the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a719f1a2-1270-4cd3-a6fd-5c0470d96b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: jupyter [-h] [--version] [--config-dir] [--data-dir] [--runtime-dir]\n",
      "               [--paths] [--json] [--debug]\n",
      "               [subcommand]\n",
      "\n",
      "Jupyter: Interactive Computing\n",
      "\n",
      "positional arguments:\n",
      "  subcommand     the subcommand to launch\n",
      "\n",
      "options:\n",
      "  -h, --help     show this help message and exit\n",
      "  --version      show the versions of core jupyter packages and exit\n",
      "  --config-dir   show Jupyter config dir\n",
      "  --data-dir     show Jupyter data dir\n",
      "  --runtime-dir  show Jupyter runtime dir\n",
      "  --paths        show all Jupyter paths. Add --json for machine-readable\n",
      "                 format.\n",
      "  --json         output paths as machine-readable json\n",
      "  --debug        output debug information about paths\n",
      "\n",
      "Available subcommands: dejavu events execute kernel kernelspec lab\n",
      "labextension labhub migrate nbconvert run server troubleshoot trust\n",
      "\n",
      "Jupyter command `jupyter-nbextension` not found.\n"
     ]
    }
   ],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3b3839-7707-401e-8046-f303101c79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"llama2-grammar/checkpoint-300\" # TODO: Put Checkpoint path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27162a17-14b1-45d8-9c2a-d000639b993d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m test_dataset  \u001b[38;5;241m=\u001b[39m load_dataset(\u001b[43mdataset\u001b[49m, split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalidation\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataset' is not defined"
     ]
    }
   ],
   "source": [
    "test_dataset  = load_dataset(dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "829a146a-73fb-4c36-81d7-c9c345b88eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5150a5f6-4961-49d2-949f-734c4db47fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c972176a37064a24b6d74af2406e8b7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_og_path,  # Mistral, same as before\n",
    "    quantization_config=bnb_config,  # Same quantization config as before\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20d928e0-d515-489a-b5f4-9e18a1e8c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404a1ec6-0174-4b10-94ed-c9fce80e350e",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Change i to change the input prompt. Evaluate the\n",
    "# outputs using your own judgement. Adjust hyperparameters above for training \n",
    "# or prompt to improve your response, and change max_new_tokens as appropriate\n",
    "\n",
    "i = 50\n",
    "max_new_tokens=75\n",
    "\n",
    "eval_prompt = f\"{bos}You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues,\" + \\\n",
    "        f\" the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {test_dataset[i]['input']} CORRECT: \" \n",
    "\n",
    "\n",
    "model_input = tokenizer(eval_prompt,m return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = tokenizer.decode(model.generate(**model_input, max_new_tokens=max_new_tokens, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7b6a0-8264-4da7-881d-0271cbcbe04a",
   "metadata": {},
   "source": [
    "# Now we have a good model. \n",
    "We have to merge our LORA's and dequantize it before we can export to Llama.cpp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9afa238f-6773-4013-814e-51fc98785dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3ac612b1-e4ef-4471-b1de-98e1e617e394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.model.embed_tokens.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "list(state_dict.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a3dab89-4846-4c54-a0db-6f3d0a8d847e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.0000, -0.6962, -0.5251, -0.3949, -0.2844, -0.1848, -0.0911,  0.0000,\n",
       "         0.0796,  0.1609,  0.2461,  0.3379,  0.4407,  0.5626,  0.7230,  1.0000],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e9e97736-3822-4fd3-80c3-e3fa36c7ed52",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8388608, 1])\n"
     ]
    }
   ],
   "source": [
    "new_tensor = F.dequantize_blockwise(\n",
    "    state_dict['base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight'],\n",
    "    None,\n",
    "    # state_dict['base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map'],\n",
    "    state_dict['base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax']\n",
    ")\n",
    "print(new_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b31dbf-85a9-4fd1-b849-c7440704a117",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8d1619da-c3e8-4275-92fc-bb4f33ae7e38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Finish me\n",
    "named_params = list(model.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "916e37e1-fdd4-4295-bc85-f570a2b11d29",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4049468423.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[29], line 2\u001b[0;36m\u001b[0m\n\u001b[0;31m    if !is_quant(param):\u001b[0m\n\u001b[0m       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "quant_params = ['absmax', 'quant_map', 'nested_absmax', 'nested_quant_map', 'quant_state.bitsandbytes__nf4']\n",
    "\n",
    "\n",
    "\n",
    "for param, _ in model.named_parameters():\n",
    "    if !is_quant(param):\n",
    "        pass\n",
    "    state_dict[param] = F.dequantize_blockwise(state_dict[param], state_dict[param + '.nested_absmax'])\n",
    "    for item in quant_params:\n",
    "        state_dict.pop(param + item)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db3efe-04c0-4d22-a5a1-e30a4495ab62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now that we have merged, we can export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8057a-7a25-4cc9-8937-799086b16a03",
   "metadata": {},
   "source": [
    "We download our weights by taring them up and using SCP\n",
    "```console\n",
    "tar -cvf models/model-final final-model.tar.gz\n",
    "```\n",
    "\n",
    "Now on your local instance, in a clone of Llama.cpp (Use my modifications here, under \"examples/llamacheck\" (https://github.com/Ferruolo/llama.cpp). Make using\n",
    "make llamacheck\n",
    "```\n",
    "scp -i myBrevInstanceName:myFirstLLM/final-model.tar.gz ./models\n",
    "tar -xvf ./models/final-model.tar.gz\n",
    "python convert-hf-to-gguf.py ./models/final-model --outfile llamacheck.gguf\n",
    "TODO: Quantize llamacheck.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd5b4e-4468-42cb-b54d-e04b8b37796a",
   "metadata": {},
   "source": [
    "# We're done. A brief reflection\n",
    "\n",
    "Being honest, my experience was not as simple as this blog post. Even though I adapted most of my code directly from the notebooks I actually used, I took a lot of twists and turns, misinterpreted docs several times, and struggled to export my weights. I hope that by compiling everything I did into one big notebook, I have made your life significantly easier! I was suprised by how easy finetuning was once I had figured everything out, but was dismayed at how difficult doing practical things with huggingface, bits and bytes library (QLORA), and llama.cpp were. I found myself in a configuration hell. There's no reason that exporting to another format should be as complicated as it was. \n",
    "\n",
    "\n",
    "I would like to thank the Brev team for helping me learn a lot of the things I talk about here, and giving me the opportunity to write this blog for them."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
