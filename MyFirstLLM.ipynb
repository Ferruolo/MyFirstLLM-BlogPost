{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10912c7a-c6c8-4bfc-a96f-e6754cecfc64",
   "metadata": {},
   "source": [
    "# My First LLM Finetune\n",
    "### A blog post by Andrew Ferruolo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbbb7a-8f17-46ae-bad4-ec52eac86e66",
   "metadata": {},
   "source": [
    "## Some Background Knowledge\n",
    "I understand not everyone reading this is going to know all the terms I throw around here. So, here is a basic dictionary to help you out\n",
    "\n",
    "Large Langauge Model (LLM) - A large AI model trained for interacting with text. ChatGPT and Claude are examples of LLMs \\\n",
    "HuggingFace - Huggingface transformers library is a package which allows us to work with LLMs while taking away a lot of the complexity. \\\n",
    "Package - Prewritten code downloaded off the internet \\\n",
    "Train/Finetune - To train an AI model means running the model over a dataset and having it make predictions, and then correcting the model parameters by assessing the correctness of its predictions. Finetuning is just continued training\\\n",
    "Llama - A LLM developer by Facebook AI research \\\n",
    "Llama.cpp - A deployment framework for llama written in C++ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8fcee-ec90-4610-a5a5-4da4c48d1f89",
   "metadata": {},
   "source": [
    "## My project\n",
    "\n",
    "If you're like me, you have to do most of your writing in C++ or Python, not in English. While I'm grateful that I get to code every day, this does lead to me making regular grammar and spelling issues. I often look at my writing and wish I could figure out how to word it better, or more naturally. I initially applied ChatGPT and Claude to this probelem, but found that they would often change my style, and even sometimes the meaning of my sentences. This is because ChatGPT and Claude are trained and managed behind closed doors, by companies who train the models to enforce their opinions on users in the name of AI saftey. While I understand that these companies need to provide some form of control and bias to prevent bad outcomes from usage of their products, I dislike having my writing altered to agree with other people's opinion. And so, I decided what I needed was to finetune an open source Large Language Model (LLM) for my purposes, and find a way to run it on my local computer. Here is a documented account of my journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208c992-5731-417e-8bc4-7b78c65c23a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirments and Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e724a0b-9ce1-4b05-942d-c69995d7d643",
   "metadata": {},
   "source": [
    "From the above paragraph, we can elicit the following requirements for my project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd71324-b5b0-486a-9e3e-0f9e55c4455d",
   "metadata": {},
   "source": [
    "1. Simple - I was a Michgan CS student at the time of the project. I didn't exactly have days to throw at this project\n",
    "2. Open Source - The goal is to get a strong understanding of LLMs, tools, frameworks, etc. \n",
    "3. Fast - If it takes 30 minutes for the LLM to run, I might as well just do the work myself.\n",
    "4. Memory Efficient - LLMs are large by nature, but my Mac only has so much RAM. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74f765-1ee9-42e0-8eb9-6fc0f3d6ea89",
   "metadata": {},
   "source": [
    "Given these requirments, I landed on the following implementation specifications:\n",
    "    \n",
    "1. Use Huggingface to train, use basic adaptation of Llama.cpp to deploy\n",
    "2. Use Llama2 7B and finetune - Easily works with Llama.cpp, and I'm a huge fan of Yann LeCun and FAIR.\n",
    "3. Weights must be quantized. Possibly, we might want to prune for a more performant and smaller network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c06bf8-4e08-4308-9fb6-c8265b2cabac",
   "metadata": {},
   "source": [
    "## We have our specs. Now, let's start building it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6369b2b-b646-490c-b407-63ed87aecbcd",
   "metadata": {},
   "source": [
    "To finetune a large language model, we need some powerful servers and GPUS. Training 7B params, even quantized, on my local computer would likely lead to it bursting into flames. So, I had to use the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78e73c-9055-4a79-a01a-d3dcae9baf8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### My first instinct: use AWS (an expensive and unfruitful journey)\n",
    "\n",
    "This, obviously, was a mistake. I don't know if you have used SageMaker before, but within a day my \"Cost and usage\" tab looked a little something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393cb3e-1efe-4fb0-ad40-c179715d8b3a",
   "metadata": {},
   "source": [
    "![AWS Cost](aws-csot.gif \"AWS COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0358dbc-5aa5-42c3-8777-ef5ab79d2f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Well. I guess it's worth it. I'm paying for an easy to use, intuitive, flexible interface right? WRONG:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e9300-a463-43c4-8bfa-b1958dbc0017",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"SageMaker-Confusing.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb804-9856-450e-8bae-fb629fbdab72",
   "metadata": {},
   "source": [
    "What even is this! I just want a basic interface. Even deleteing profiles, instances, and the rest from AWS is a huge pain. Although I'm sure that at the enterprise level there is a good reason for all of this, I personally don't want to deal with it. So, I'm just not going to. Bye Jeff!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efcf75-d796-4b62-a031-5412d397311a",
   "metadata": {},
   "source": [
    "### A Better Solution: Brev.dev\n",
    "\n",
    "I then remembered a company called Brev.dev, which I had seen on Twitter a couple months ago. After checking them out, I discovered they agreed with me on how AWS is nearly unuseable, and created a solution for AI hackers like me. Their simple interface allowed me to complete the rest of my project in just a few hours, and for minimal cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84245e7e-e3f9-4e23-be0a-438fa299b8e1",
   "metadata": {},
   "source": [
    "### Spinning up is simple\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"Brev-Spinup.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Managing is just as simple\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-start.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### SSH is easier than ever before\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-ssh.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c7f62-e5a9-41e5-b2a5-067522f1e9fa",
   "metadata": {},
   "source": [
    "### Now that's what we like to see\n",
    "Brev provides a simple, clean interface to get new GPUS. Also, they're insanely cheap. Look at those prices! Finetuning might cost me less than a burger if I do it right! Now that we have our instance all set up, lets start actually doing work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139968e-648c-41de-9188-4a7e9fc543cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Getting my dataset/model ready for huggingface:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2450-9515-4bde-add7-6a99962c8740",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Create directory called \"grammar_dataset, with two subdirectories called \"train\" and \"validation\"\n",
    "2. Download data using \"download_grammar_dataset.py\"\n",
    "3. Move \"gtrain_10k.csv\" to train, \"grammar_validation.csv\" to validation\n",
    "4. Add a readme to grammar dataset, with following as follows: (copy directly from cell below, between the two quotes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5534bfa-3a87-4161-98da-b9900887d4b5",
   "metadata": {},
   "source": [
    "```console\n",
    "---\n",
    "configs:\n",
    "- config_name: default\n",
    "  data_files:\n",
    "  - split: train\n",
    "    path: \"train/gtrain_10k.csv\"\n",
    "  - split: validation\n",
    "    path: \"validation/grammar_validation.csv\"\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98659e3c-6099-4d07-9bea-eb809e5f530e",
   "metadata": {},
   "source": [
    "Now we need to get our model\n",
    "\n",
    "5. Visit this link (https://llama.meta.com/llama-downloads/), and follow the instructions to download your desired model to your instance\n",
    "6. Use the \"convert_llama_weights_to_hf.py\" script in this repo to convert your weights to huggingface format\n",
    "7. If your filetree looks like like the one below, your're ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5697c9-a1df-4448-9473-a43e2a647fca",
   "metadata": {},
   "source": [
    "```console\n",
    ".\n",
    "├── MyFirstLLM.ipynb\n",
    "├── datasets\n",
    "│   └── grammar_dataset\n",
    "│       ├── README.md\n",
    "│       ├── train\n",
    "│       │   └── gtrain_10k.csv\n",
    "│       └── validation\n",
    "│           └── grammar_validation.csv\n",
    "├── models\n",
    "│   ├── convert_llama_weights_to_hf.py\n",
    "│   ├── llama-7B-huggingface\n",
    "│   │   ├── config.json\n",
    "│   │   ├── generation_config.json\n",
    "│   │   ├── pytorch_model-00001-of-00003.bin\n",
    "│   │   ├── pytorch_model-00002-of-00003.bin\n",
    "│   │   ├── pytorch_model-00003-of-00003.bin\n",
    "│   │   ├── pytorch_model.bin.index.json\n",
    "│   │   ├── special_tokens_map.json\n",
    "│   │   ├── tokenizer.json\n",
    "│   │   ├── tokenizer.model\n",
    "│   │   └── tokenizer_config.json\n",
    "│   └── llama-7B-pytorch\n",
    "│       ├── checklist.chk\n",
    "│       ├── consolidated.00.pth\n",
    "│       ├── params.json\n",
    "│       └── tokenizer.model\n",
    "└── readme.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c487d7-9e93-40b4-9c68-ee7ec654bc46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning my model, using huggingface."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5a5e8-5a9e-4e08-beb2-532e50791f5e",
   "metadata": {},
   "source": [
    "Warning: This part might get a little dense. You'll have to forgive me if I breeze over some explanations. For a deeper explanation, take a look at Harper Carroll's blog here: (https://brev.dev/blog/how-qlora-works). The majority of this section is adapted from the given blog, with adjustments added in as I saw fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a743bc1-10b5-48f4-ad34-95b41d2d4fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install Eveything\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip instal bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534ea79f-bf63-42f9-b536-9c32d7f944ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6bd6d7-d025-4f7c-b8a4-d2b8fc97025c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "llama_og_path = \"./models/llama-7B-huggingface\"\n",
    "llama_token_path = \"./models/llama-7B-huggingface\"\n",
    "dataset = \"./datasets/grammar_dataset/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d48964-3a0b-4ae4-911a-37b5ac5e6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = load_dataset(dataset, split='train')\n",
    "eval_dataset  = load_dataset(dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "98cdc1c9-94ad-47a6-b305-3c0cef124cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    model_max_length=256,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d553a4f-25a3-4174-8bc7-8e3334bf3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    target = data_point['input']\n",
    "    result = data_point['target']\n",
    "    \n",
    "    full_prompt = f\"You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, \\\n",
    "        the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {target} CORRECT: {result}\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f17af-68d0-4e42-b7d8-884a02b1e62f",
   "metadata": {},
   "source": [
    "### Prep Model For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac77b7b2-26a0-48fe-ab22-58a5a0502d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c34e8ab-7118-47dc-82e1-191213a2c6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494a2129c83c4f0b94b39850ac73a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_og_path, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356a7ca4-a176-41cf-82b3-95e1157da041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The University of Michgian 2018-19 Men's Basketball News\n",
      " nobody can stop the Wolverines\n"
     ]
    }
   ],
   "source": [
    "# Re-init the tokenizer so it doesn't add padding or eos token\n",
    "eval_prompt = \"The University of Michgian \" # GO BLUE!\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    padding_side=\"left\",\n",
    "    model_max_length=20,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=20)[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4b4610-aa17-4e02-baaa-873b12d0d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a47d61f1-30ed-458d-be52-b1be02381341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc324fc-5e36-4e39-a58a-5873f5888e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15207936 || all params: 3515620864 || trainable%: 0.4325817995828119\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)\n",
    "\n",
    "# model = acelerator.prepare_model(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d48f241-d051-4c42-9fbe-e484d57a28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f353b5-87df-4a26-b164-607395e7db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"grammar\"\n",
    "base_model_name = \"llama2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",   \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,                # Save checkpoints every 50 steps\n",
    "        evaluation_strategy=\"steps\", # Evaluate the model every 50 logging step\n",
    "        eval_steps=50,               # Evaluate and save checkpoints every 50 steps\n",
    "        do_eval=True,                # Perform evaluation at the end of training\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705f5f3-4521-4ae5-b6fb-b3000068c395",
   "metadata": {},
   "source": [
    "You'll get an output that looks like this: (training SS).\n",
    "\n",
    "\n",
    "\n",
    "Run until your Validation Loss is no longer decreasing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b0a87-1391-48ee-88f8-0726067729a8",
   "metadata": {},
   "source": [
    "# We Trained, Now What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5b3a0-a1f4-4214-aa1a-1783a6787656",
   "metadata": {},
   "source": [
    "## First lets test the model\n",
    "1. Hit ESC-00 (reset the kernel), then evaluate by hand using the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a719f1a2-1270-4cd3-a6fd-5c0470d96b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ef82b9-eb44-4668-bd98-82ca31475d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "llama_og_path = \"./models/llama-7B-huggingface\"\n",
    "llama_token_path = \"./models/llama-7B-huggingface\"\n",
    "dataset = \"./datasets/grammar_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e3b3839-7707-401e-8046-f303101c79bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = \"llama2-grammar/checkpoint-300\" # TODO: Put Checkpoint path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27162a17-14b1-45d8-9c2a-d000639b993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = load_dataset(dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829a146a-73fb-4c36-81d7-c9c345b88eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5150a5f6-4961-49d2-949f-734c4db47fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71791c27eaa94dea8ef7d5df30547a99",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_og_path, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d928e0-d515-489a-b5f4-9e18a1e8c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = PeftModel.from_pretrained(base_model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c347058-39e9-4eb3-a893-f1b61c436e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: So, if i have alot of information about this subject, i will taulk too much with knowledge but if i have general information for this subject, i will talk about this subjec with my limited knowlege and this case may be make me shame like when my brother asked me about some thing but i have not alot of information about this thing. CORRECT: 1) If I have a lot of information about this topic, I’ll speak too much with confidence; however, if I only have general information on it, I might feel ashamed to talk about it because I don’t want to appear as though I am speaking out of turn.\n",
      "You will see two sentences. The first is marked INCORRECT and\n"
     ]
    }
   ],
   "source": [
    "# Change i to change the input prompt. Evaluate the\n",
    "# outputs using your own judgement. Adjust hyperparameters above for training \n",
    "# or prompt to improve your response, and change max_new_tokens as appropriate\n",
    "\n",
    "i = 50\n",
    "max_new_tokens=75\n",
    "\n",
    "eval_prompt = f\"{bos}You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues,\" + \\\n",
    "        f\" the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {test_dataset[i]['input']} CORRECT: \" \n",
    "\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = tokenizer.decode(model.generate(**model_input, max_new_tokens=max_new_tokens, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7b6a0-8264-4da7-881d-0271cbcbe04a",
   "metadata": {},
   "source": [
    "# Exporting The Model \n",
    "We SHOULD  be happy with the performance of the previous model, and want to move it to our local computer (For me, this is a macbook). Bits and Bytes doesn't provide a way to export our weights back to float32 in a convienient way. So, we will have to write it ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48c42d78-1e30-409f-883a-9ba36919a863",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bd4ec42b8ff488bb92918fd667a521c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load original weights to make sure we get the right interface. This time we don't quantize\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_og_path, \n",
    "    device_map=\"cpu\", # Can't fit it all on the GPU. I'm using 32 gigs of ram\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "68396720-144a-4fa8-b370-3cc2acffb55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_state_dict = target_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0c2353ad-e663-41cc-9065-21392bcbf0c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "probe_weight = target_state_dict['model.layers.0.self_attn.q_proj.weight']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9afa238f-6773-4013-814e-51fc98785dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3ac612b1-e4ef-4471-b1de-98e1e617e394",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['base_model.model.model.embed_tokens.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight.absmax',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight.quant_map',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight.nested_absmax',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight.nested_quant_map',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.base_layer.weight.quant_state.bitsandbytes__nf4',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight',\n",
       " 'base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight',\n",
       " 'base_model.model.model.layers.0.input_layernorm.weight',\n",
       " 'base_model.model.model.layers.0.post_attention_layernorm.weight',\n",
       " 'base_model.model.model.layers.1.self_attn.q_proj.base_layer.weight']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state_dict = model.state_dict()\n",
    "list(state_dict.keys())[:60]  # Commented out to truncate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "213d98ab-437e-4bdd-9c38-7d45c994246d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bitsandbytes.functional.QuantState at 0x7f3aad1ed5a0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.base_model.model.model.layers[0].self_attn.q_proj.base_layer.weight.quant_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "57086107-c5fb-4761-87ec-442f0599ea9f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['model.embed_tokens.weight',\n",
       " 'model.layers.0.self_attn.q_proj.weight',\n",
       " 'model.layers.0.self_attn.k_proj.weight',\n",
       " 'model.layers.0.self_attn.v_proj.weight',\n",
       " 'model.layers.0.self_attn.o_proj.weight',\n",
       " 'model.layers.0.mlp.gate_proj.weight',\n",
       " 'model.layers.0.mlp.up_proj.weight',\n",
       " 'model.layers.0.mlp.down_proj.weight',\n",
       " 'model.layers.0.input_layernorm.weight',\n",
       " 'model.layers.0.post_attention_layernorm.weight',\n",
       " 'model.layers.1.self_attn.q_proj.weight',\n",
       " 'model.layers.1.self_attn.k_proj.weight',\n",
       " 'model.layers.1.self_attn.v_proj.weight',\n",
       " 'model.layers.1.self_attn.o_proj.weight',\n",
       " 'model.layers.1.mlp.gate_proj.weight',\n",
       " 'model.layers.1.mlp.up_proj.weight',\n",
       " 'model.layers.1.mlp.down_proj.weight',\n",
       " 'model.layers.1.input_layernorm.weight',\n",
       " 'model.layers.1.post_attention_layernorm.weight',\n",
       " 'model.layers.2.self_attn.q_proj.weight']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(target_state_dict.keys())[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b4e5ccb-df09-464d-939f-7a3293d5eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_quant(name, sd) -> bool:\n",
    "    return name + '.nested_absmax' in sd.keys()\n",
    "\n",
    "\n",
    "def is_lora(name, sd) -> bool:\n",
    "    return name[:-18] + '.lora_A.default.weight' in sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4335f544-eda4-48bf-9844-1b363ba37a8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_state_dict[\"model.layers.0.self_attn.q_proj.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5ecd31d-6107-4063-b72a-7629933795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 0 #32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dce4cd-dc99-479a-87be-0932f4268665",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in range(num_layers):\n",
    "    weights = [\n",
    "        # Can probably be done way cleaner, feel free to put in a pull request\n",
    "        (model.base_model.model.model.layers[layer].self_attn.q_proj.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.self_attn.q_proj.base_layer.weight',\n",
    "         f'model.layers.{layer}.self_attn.q_proj.weight'\n",
    "        ),\n",
    "        (model.base_model.model.model.layers[layer].self_attn.k_proj.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.self_attn.k_proj.base_layer.weight',\n",
    "         f'model.layers.{layer}.self_attn.k_proj.weight',\n",
    "        ),\n",
    "        (model.base_model.model.model.layers[layer].self_attn.v_proj.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.self_attn.v_proj.base_layer.weight',\n",
    "         f'model.layers.{layer}.self_attn.v_proj.weight',\n",
    "        ),\n",
    "        (model.base_model.model.model.layers[layer].self_attn.o_proj.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.self_attn.o_proj.base_layer.weight',\n",
    "         f'model.layers.{layer}.self_attn.o_proj.weight',\n",
    "        ),\n",
    "        (model.base_model.model.model.layers[layer].mlp.gate_proj.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.mlp.gate_proj.weight',\n",
    "         f'model.layers.{layer}.mlp.gate_proj.weight'\n",
    "        ),\n",
    "        (model.base_model.model.model.layers[layer].mlp.up_proj.weight.quant_state,   \n",
    "         f'base_model.model.model.layers.{layer}.mlp.up_proj.weight',\n",
    "        \n",
    "        \n",
    "        ),\n",
    "        (model.base_model.model.model.layers[layer].mlp.down_proj.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.mlp.down_proj.weight'),\n",
    "        (model.base_model.model.model.layers[layer].input_layernorm.weight.quant_state,\n",
    "         f'base_model.model.model.layers.{layer}.input_layernorm.weight'),\n",
    "        (model.base_model.model.model.layers[layer].post_attention_layernorm.weight.quant_state, \n",
    "         f'base_model.model.model.layers.{layer}.self_attn.q_proj.base_layer.weight'),\n",
    "    ]\n",
    "    for weight_class, key in weights:\n",
    "        W = state_dict[key]\n",
    "        if is_quant(key, state_dict):\n",
    "            lay_num = int(re.search(r'\\d+', string).group())\n",
    "            q_state = model.base_model.model.model.layers[lay_num].self_attn.q_proj.base_layer.weight.quant_state\n",
    "            W = F.dequantize_4bit(state_dict[base_lay_name], q_state)\n",
    "    \n",
    "        if is_lora(key, state_dict):    \n",
    "            A = state_dict[lora_a_name]\n",
    "            B = state_dict[lora_b_name]\n",
    "            delta_W = torch.matmul(A, B)\n",
    "            W = W + delta_W\n",
    "        \n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87341c6d-0f8f-4466-aa06-7f5c2c5e06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = 0\n",
    "weight = model.base_model.model.model.layers[layer].self_attn.q_proj.base_layer.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8ce40c2e-d7bb-489c-b350-fcff7a331279",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "Parameter(Params4bit([[ 83],\n",
       "            [103],\n",
       "            [ 74],\n",
       "            ...,\n",
       "            [114],\n",
       "            [108],\n",
       "            [181]], device='cuda:0', dtype=torch.uint8))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b44b74-f42d-4479-a7d5-e16dd8eaa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "for target_lay_name, _ in target_state_dict.items():\n",
    "    target_lay_name_clean = target_lay_name[:-7]\n",
    "\n",
    "    base_lay_name = 'base_model.model.' + target_lay_name_clean + '.base_layer.weight'\n",
    "    absmax_name = 'base_model.model.' + target_lay_name_clean + '.base_layer.weight.nested_absmax'\n",
    "    lora_a_name = 'base_model.model.' + target_lay_name_clean  + '.lora_A.default.weight'\n",
    "    lora_b_name = 'base_model.model.' + target_lay_name_clean  + '.lora_B.default.weight'\n",
    "    state_name = 'base_model.model.' + target_lay_name_clean  + '.base_layer.weight.quant_state.bitsandbytes__nf4'\n",
    "\n",
    "    if base_lay_name not in state_dict.keys():\n",
    "        continue\n",
    "\n",
    "    \n",
    "    W = state_dict[base_lay_name]\n",
    "    if is_quant(target_lay_name_clean, state_dict):\n",
    "        lay_num = int(re.search(r'\\d+', string).group())\n",
    "        q_state = model.base_model.model.model.layers[lay_num].self_attn.q_proj.base_layer.weight.quant_state\n",
    "        W = F.dequantize_4bit(state_dict[base_lay_name], q_state)\n",
    "\n",
    "    if is_lora(target_lay_name, state_dict):    \n",
    "        A = state_dict[lora_a_name]\n",
    "        B = state_dict[lora_b_name]\n",
    "        delta_W = torch.matmul(A, B)\n",
    "        W = W + delta_W\n",
    "\n",
    "    W.to('cpu')\n",
    "    \n",
    "    target_state_dict[target_lay_name] = W\n",
    "    print(f\"Merged {target_lay_name}\")\n",
    "    \n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    # layers = [\n",
    "    #     base_lay_name,\n",
    "    #     absmax_name,\n",
    "    #     lora_a_name,\n",
    "    #     lora_b_name,\n",
    "    #     state_name\n",
    "    # ]\n",
    "    # for item in layers:\n",
    "    #     tensor = state_dict[item]\n",
    "    #     state_dict.pop(item)\n",
    "    #     del tensor\n",
    "\n",
    "    # torch.cuda.synchronize()\n",
    "    # torch.cuda.empty_cache() \n",
    "    # assert(base_lay_name not in state_dict.keys())\n",
    "              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ab6e9182-f140-420d-bde9-6e1b91f949a4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4096, 4096])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_state_dict[\"model.layers.0.self_attn.q_proj.weight\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "88f146ae-3449-449a-8b66-d3bf634891ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "972199d7-2b6f-4c5c-8619-1772894fadcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "33726397-f83f-4fae-abd0-b4353d86ee2c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.22.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.22.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.23.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.23.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.24.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.24.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.25.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.25.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.26.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.26.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.27.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.27.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.28.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.28.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.28.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.29.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.29.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.30.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.30.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.31.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.31.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.31.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[26], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m target_model \u001b[38;5;241m=\u001b[39m \u001b[43mtarget_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtarget_state_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:2153\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2148\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2149\u001b[0m             \u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2150\u001b[0m                 \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)))\n\u001b[1;32m   2152\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2153\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2154\u001b[0m                        \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)))\n\u001b[1;32m   2155\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LlamaForCausalLM:\n\tsize mismatch for model.layers.0.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.0.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.0.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.1.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.1.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.1.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.2.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.2.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.2.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.3.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.3.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.3.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.4.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.4.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.4.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.5.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.5.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.5.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.6.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.6.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.6.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.7.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.7.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.7.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.8.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.8.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.8.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.9.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.9.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.9.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.10.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.10.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.10.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.11.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.11.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.11.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.12.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.12.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.12.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.13.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.13.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.13.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.14.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.14.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.14.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.15.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.15.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.15.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.16.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.16.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.16.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.17.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.17.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.17.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.18.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.18.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.18.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.19.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.19.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.19.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.20.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.20.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.20.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.21.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.21.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.21.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.22.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.22.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.22.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.23.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.23.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.23.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.24.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.24.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.24.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.25.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.25.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.25.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.26.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.26.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.26.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.27.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.27.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.27.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.28.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.28.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.28.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.29.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.29.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.29.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.30.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.30.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.30.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008]).\n\tsize mismatch for model.layers.31.mlp.gate_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.31.mlp.up_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([11008, 4096]).\n\tsize mismatch for model.layers.31.mlp.down_proj.weight: copying a param with shape torch.Size([4096, 4096]) from checkpoint, the shape in current model is torch.Size([4096, 11008])."
     ]
    }
   ],
   "source": [
    "target_model = target_model.load_state_dict(target_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db3efe-04c0-4d22-a5a1-e30a4495ab62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now that we have merged, we can export"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8057a-7a25-4cc9-8937-799086b16a03",
   "metadata": {},
   "source": [
    "We download our weights by taring them up and using SCP\n",
    "```console\n",
    "tar -cvf models/model-final final-model.tar.gz\n",
    "```\n",
    "\n",
    "Now on your local instance, in a clone of Llama.cpp (Use my modifications here, under \"examples/llamacheck\" (https://github.com/Ferruolo/llama.cpp). Make using\n",
    "make llamacheck\n",
    "```\n",
    "scp -i myBrevInstanceName:myFirstLLM/final-model.tar.gz ./models\n",
    "tar -xvf ./models/final-model.tar.gz\n",
    "python convert-hf-to-gguf.py ./models/final-model --outfile llamacheck.gguf\n",
    "TODO: Quantize llamacheck.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd5b4e-4468-42cb-b54d-e04b8b37796a",
   "metadata": {},
   "source": [
    "# We're done. A brief reflection\n",
    "\n",
    "Being honest, my experience was not as simple as this blog post. Even though I adapted most of my code directly from the notebooks I actually used, I took a lot of twists and turns, misinterpreted docs several times, and struggled to export my weights. I hope that by compiling everything I did into one big notebook, I have made your life significantly easier! I was suprised by how easy finetuning was once I had figured everything out, but was dismayed at how difficult doing practical things with huggingface, bits and bytes library (QLORA), and llama.cpp were. I found myself in a configuration hell. There's no reason that exporting to another format should be as complicated as it was. \n",
    "\n",
    "\n",
    "I would like to thank the Brev team for helping me learn a lot of the things I talk about here, and giving me the opportunity to write this blog for them."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47a24d2-b8fe-4873-b87c-1d59ec50dc66",
   "metadata": {},
   "source": [
    "If you have any followup questions, or would like to reach out to me for another reason, please contact my email at andrew.ferruolo@gmail.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeb072d-4ab1-4d04-9b75-9c3111f267d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
