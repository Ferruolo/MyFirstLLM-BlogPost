{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "10912c7a-c6c8-4bfc-a96f-e6754cecfc64",
   "metadata": {},
   "source": [
    "# My First LLM Finetune\n",
    "### A Blog Post by Andrew Ferruolo\n",
    "Post Located at https://github.com/Ferruolo/MyFirstLLM-BlogPost"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cbbb7a-8f17-46ae-bad4-ec52eac86e66",
   "metadata": {},
   "source": [
    "## Some Background Knowledge\n",
    "I understand not everyone reading this is going to know all the terms I throw around here. So, here is a basic dictionary to help you out\n",
    "\n",
    "Large Langauge Model (LLM) - A large AI model trained for interacting with text. ChatGPT and Claude are examples of LLMs \\\n",
    "HuggingFace - Huggingface transformers library is a package which allows us to work with LLMs while taking away a lot of the complexity \\\n",
    "Package - Prewritten code downloaded off the internet \\\n",
    "Parameter - Numbers that an AI model uses to determine its output \\\n",
    "Train/Finetune - To train an AI model means running the model over a dataset and having it make predictions, and then correcting the model parameters by assessing the correctness of its predictions. Finetuning is just continued training\\\n",
    "Llama - A LLM developer by Facebook AI research \\\n",
    "Llama.cpp - A deployment framework for llama written in C++ \n",
    "Hyperparameter - A parameter that you set which controls how the model is trained \\\n",
    "Open Source - Code available publicly on the internet for anyone to use or download"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be8fcee-ec90-4610-a5a5-4da4c48d1f89",
   "metadata": {},
   "source": [
    "## My Project\n",
    "\n",
    "If you're like me, you have to do most of your writing in C++ or Python, not in English. While I'm grateful that I get to code every day, this leads me to make regular grammar and spelling mistakes. I often look at my writing and wish I could figure out how to word it better, or more naturally. I initially applied ChatGPT and Claude for this problem, but found that they would often change my style, and sometimes even the meaning of my sentences. This is because ChatGPT and Claude are trained and managed behind closed doors in the name of AI Safety, leaving the models with the biases of its creators. While I understand that these companies need to provide some form of control and bias to prevent bad outcomes from usage of their products, this can occasionally lead to some of the trainers' biases altering my writing, which I dislike. And so, I decided that I needed to finetune an Open Source Large Language Model (LLM), and find a way to run it on my local computer. Here is a documented account of my journey."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b208c992-5731-417e-8bc4-7b78c65c23a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Requirments and Specifications"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e724a0b-9ce1-4b05-942d-c69995d7d643",
   "metadata": {},
   "source": [
    "From the above paragraph, we can elicit the following requirements for my project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdd71324-b5b0-486a-9e3e-0f9e55c4455d",
   "metadata": {},
   "source": [
    "1. Simple - I was a Michgan CS student at the time of the project. I didn't exactly have days to throw at this project\n",
    "2. Open Source - The goal is to get a strong understanding of LLMs, tools, frameworks, etc \n",
    "3. Fast - If it takes 30 minutes for the LLM to run, I might as well just do the work myself\n",
    "4. Memory Efficient - LLMs are large by nature, but my Mac only has so much RAM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f74f765-1ee9-42e0-8eb9-6fc0f3d6ea89",
   "metadata": {},
   "source": [
    "Given these requirments, I landed on the following implementation specifications:\n",
    "    \n",
    "1. Use Huggingface to train, use basic adaptation of Llama.cpp to deploy\n",
    "2. Use Llama2 7B and finetune - Easily works with Llama.cpp, and I'm a huge fan of Yann LeCun and Facebook AI Research\n",
    "3. Weights must be quantized. Possibly, we might want to prune for a more performant and smaller network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c06bf8-4e08-4308-9fb6-c8265b2cabac",
   "metadata": {},
   "source": [
    "## Let's Start Building!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6369b2b-b646-490c-b407-63ed87aecbcd",
   "metadata": {},
   "source": [
    "To finetune a large language model, we need some powerful servers and GPUS. Training 7B params, even quantized, on my local computer would likely lead to it bursting into flames. So, I had to use the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e78e73c-9055-4a79-a01a-d3dcae9baf8a",
   "metadata": {
    "tags": []
   },
   "source": [
    "### My First Instinct: Use AWS\n",
    "\n",
    "This, obviously, was a mistake. I don't know if you have used SageMaker before, but within a day my \"Costs and Usage\" tab looked something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9393cb3e-1efe-4fb0-ad40-c179715d8b3a",
   "metadata": {},
   "source": [
    "![AWS Cost](aws-csot.gif \"AWS COST\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0358dbc-5aa5-42c3-8777-ef5ab79d2f2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Even worse, the interface made it clear that I was not the target audience"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "350e9300-a463-43c4-8bfa-b1958dbc0017",
   "metadata": {
    "tags": []
   },
   "source": [
    "\n",
    "<div>\n",
    "<img src=\"SageMaker-Confusing.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3deb804-9856-450e-8bae-fb629fbdab72",
   "metadata": {},
   "source": [
    "This interface is designed for AWS's target audience, enterprise companies. As I am not an enterprise company, I found their set up a little complicated, and decided to find an alternative."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69efcf75-d796-4b62-a031-5412d397311a",
   "metadata": {},
   "source": [
    "### A Better Solution: Brev.dev\n",
    "\n",
    "I then remembered a company called Brev.dev, which I had seen on Twitter a couple months ago. After checking them out, I discovered they had created a solution for AI hackers like me. Their simple interface allowed me to complete the rest of my project pretty fast, and for minimal cost."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84245e7e-e3f9-4e23-be0a-438fa299b8e1",
   "metadata": {},
   "source": [
    "### Spinning Up is Simple\n",
    "\n",
    "\n",
    "<div>\n",
    "<img src=\"Brev-Spinup.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### Managing is Just as Simple\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-start.jpg\" width=\"500\"/>\n",
    "</div>\n",
    "\n",
    "### SSH is Easier Than Ever Before\n",
    "\n",
    "<div>\n",
    "<img src=\"brev-ssh.jpg\" width=\"500\"/>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "961c7f62-e5a9-41e5-b2a5-067522f1e9fa",
   "metadata": {},
   "source": [
    "### I'm Lovin It!\n",
    "Brev provides a simple, clean interface to get GPUS. Also, they're pretty cheap! Finetuning might cost me less than a burger if I do it right! Now that we have our instance all set up, lets start actually doing work"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc220f1b-cbaa-4421-88bc-19f72a25fe7a",
   "metadata": {},
   "source": [
    "(I realize that was a bit of a shill right there, but those are my genuine opinions!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1139968e-648c-41de-9188-4a7e9fc543cb",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Prep Model For Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ed2450-9515-4bde-add7-6a99962c8740",
   "metadata": {},
   "source": [
    "### Steps\n",
    "1. Create directory called \"grammar_dataset, with two subdirectories called \"train\" and \"validation\"\n",
    "2. Download data using \"download_grammar_dataset.py\"\n",
    "3. Move \"gtrain_10k.csv\" to train directory, \"grammar_validation.csv\" to validation directory\n",
    "4. Add a readme to grammar dataset, as follows: (copy all visible text from below, do not copy backticks and 'console')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5534bfa-3a87-4161-98da-b9900887d4b5",
   "metadata": {},
   "source": [
    "```console\n",
    "---\n",
    "configs:\n",
    "- config_name: default\n",
    "  data_files:\n",
    "  - split: train\n",
    "    path: \"train/gtrain_10k.csv\"\n",
    "  - split: validation\n",
    "    path: \"validation/grammar_validation.csv\"\n",
    "---\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98659e3c-6099-4d07-9bea-eb809e5f530e",
   "metadata": {},
   "source": [
    "Now we need to get our model\n",
    "\n",
    "5. Visit this link (https://llama.meta.com/llama-downloads/), and follow the instructions to download your desired model to your instance\n",
    "6. Use the \"convert_llama_weights_to_hf.py\" script in this repo to convert your weights to HuggingFace format\n",
    "7. If your filetree looks like like the one below, your're ready to go!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5697c9-a1df-4448-9473-a43e2a647fca",
   "metadata": {},
   "source": [
    "```console\n",
    ".\n",
    "├── MyFirstLLM.ipynb\n",
    "├── datasets\n",
    "│   └── grammar_dataset\n",
    "│       ├── README.md\n",
    "│       ├── train\n",
    "│       │   └── gtrain_10k.csv\n",
    "│       └── validation\n",
    "│           └── grammar_validation.csv\n",
    "├── models\n",
    "│   ├── convert_llama_weights_to_hf.py\n",
    "│   ├── llama-7B-huggingface\n",
    "│   │   ├── config.json\n",
    "│   │   ├── generation_config.json\n",
    "│   │   ├── pytorch_model-00001-of-00003.bin\n",
    "│   │   ├── pytorch_model-00002-of-00003.bin\n",
    "│   │   ├── pytorch_model-00003-of-00003.bin\n",
    "│   │   ├── pytorch_model.bin.index.json\n",
    "│   │   ├── special_tokens_map.json\n",
    "│   │   ├── tokenizer.json\n",
    "│   │   ├── tokenizer.model\n",
    "│   │   └── tokenizer_config.json\n",
    "│   └── llama-7B-pytorch\n",
    "│       ├── checklist.chk\n",
    "│       ├── consolidated.00.pth\n",
    "│       ├── params.json\n",
    "│       └── tokenizer.model\n",
    "└── readme.md\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c487d7-9e93-40b4-9c68-ee7ec654bc46",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Finetuning Using Huggingface"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e201c74-efa4-490c-9e81-04b9620ed448",
   "metadata": {},
   "source": [
    "If you are planning on using this exact notebook to train yourself, I used an instance with an A10G GPU with 24gb VRAM, 240gb disk space, 32gb ram, and 8 CPU cores. This instance should be easily available on Brev. You could also use an A100, even though it might be a bit overkill. As long as you meet the memory requirements of 24gb on GPU and 32gb RAM, you should be chilling. As for containers, I had the best results using the container with PyTorch and cuDNN already built in"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1d5a5e8-5a9e-4e08-beb2-532e50791f5e",
   "metadata": {},
   "source": [
    "Warning: This part might get a little dense. You'll have to forgive me if I breeze over some explanations. For a deeper explanation, take a look at Harper Carroll's blog here: (https://brev.dev/blog/how-qlora-works). The majority of this section is adapted from the Harper's blog, with adjustments added in as I saw fit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a743bc1-10b5-48f4-ad34-95b41d2d4fbb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Install Eveything\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip instal bitsandbytes\n",
    "!pip install peft\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "534ea79f-bf63-42f9-b536-9c32d7f944ac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e6bd6d7-d025-4f7c-b8a4-d2b8fc97025c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "llama_og_path = \"./models/llama-7B-huggingface\"\n",
    "llama_token_path = \"./models/llama-7B-huggingface\"\n",
    "dataset = \"./datasets/grammar_dataset/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "57d48964-3a0b-4ae4-911a-37b5ac5e6ba6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Datasets\n",
    "train_dataset = load_dataset(dataset, split='train')\n",
    "eval_dataset  = load_dataset(dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98cdc1c9-94ad-47a6-b305-3c0cef124cf1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    model_max_length=256,\n",
    "    padding_side=\"left\",\n",
    "    add_eos_token=True)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.bos_token\n",
    "def tokenize(prompt):\n",
    "    result = tokenizer(\n",
    "        prompt,\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d553a4f-25a3-4174-8bc7-8e3334bf3814",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_and_tokenize_prompt(data_point):\n",
    "    target = data_point['input']\n",
    "    result = data_point['target']\n",
    "    \n",
    "    full_prompt = f\"You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, \\\n",
    "        the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {target} CORRECT: {result}\"\n",
    "    return tokenize(full_prompt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220f17af-68d0-4e42-b7d8-884a02b1e62f",
   "metadata": {},
   "source": [
    "### Prep Model For Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac77b7b2-26a0-48fe-ab22-58a5a0502d5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map Prompts to Datasets\n",
    "tokenized_train_dataset = train_dataset.map(generate_and_tokenize_prompt)\n",
    "tokenized_val_dataset = eval_dataset.map(generate_and_tokenize_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c34e8ab-7118-47dc-82e1-191213a2c6a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "494a2129c83c4f0b94b39850ac73a905",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(llama_og_path, quantization_config=bnb_config, device_map=\"auto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "356a7ca4-a176-41cf-82b3-95e1157da041",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> The University of Michgian 2018-19 Men's Basketball News\n",
      " nobody can stop the Wolverines\n"
     ]
    }
   ],
   "source": [
    "# Make sure the model works as expected\n",
    "eval_prompt = \"The University of Michgian \" # GO BLUE!\n",
    "eval_tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path,\n",
    "    padding_side=\"left\",\n",
    "    model_max_length=20,\n",
    ")\n",
    "\n",
    "model_input = eval_tokenizer(eval_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    print(eval_tokenizer.decode(model.generate(**model_input, max_new_tokens=20)[0], skip_special_tokens=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4e4b4610-aa17-4e02-baaa-873b12d0d204",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.gradient_checkpointing_enable()\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a47d61f1-30ed-458d-be52-b1be02381341",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params} || all params: {all_param} || trainable%: {100 * trainable_params / all_param}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4fc324fc-5e36-4e39-a58a-5873f5888e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 15207936 || all params: 3515620864 || trainable%: 0.4325817995828119\n"
     ]
    }
   ],
   "source": [
    "config = LoraConfig(\n",
    "    r=6,\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\n",
    "        \"q_proj\",\n",
    "        \"k_proj\",\n",
    "        \"v_proj\",\n",
    "        \"o_proj\",\n",
    "        \"gate_proj\",\n",
    "        \"up_proj\",\n",
    "        \"down_proj\",\n",
    "        \"lm_head\",\n",
    "    ],\n",
    "    bias=\"none\",\n",
    "    lora_dropout=0.05,\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)\n",
    "print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d48f241-d051-4c42-9fbe-e484d57a28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if torch.cuda.device_count() > 1: # If more than 1 GPU\n",
    "    model.is_parallelizable = True\n",
    "    model.model_parallel = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f353b5-87df-4a26-b164-607395e7db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "from datetime import datetime\n",
    "\n",
    "project = \"grammar\"\n",
    "base_model_name = \"llama2\"\n",
    "run_name = base_model_name + \"-\" + project\n",
    "output_dir = \"./\" + run_name\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=tokenized_train_dataset,\n",
    "    eval_dataset=tokenized_val_dataset,\n",
    "    args=transformers.TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        warmup_steps=5,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_checkpointing=True,\n",
    "        gradient_accumulation_steps=4,\n",
    "        max_steps=1000,\n",
    "        learning_rate=2.5e-5,\n",
    "        logging_steps=50,\n",
    "        bf16=True,\n",
    "        optim=\"paged_adamw_8bit\",\n",
    "        logging_dir=\"./logs\",   \n",
    "        save_strategy=\"steps\",\n",
    "        save_steps=50,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=50,\n",
    "        do_eval=True,\n",
    "    ),\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False),\n",
    ")\n",
    "\n",
    "model.config.use_cache = False\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c705f5f3-4521-4ae5-b6fb-b3000068c395",
   "metadata": {},
   "source": [
    "Allow the above cell to run until your validation loss (this will be labeled in the output) has stopped decreassing. This means your model is done training, and any additional training will just lead to greater inaccuracy on real world problems due to what we call overfitting. Write down (or mark in this notebook) which checkpoint did the best"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069b0a87-1391-48ee-88f8-0726067729a8",
   "metadata": {},
   "source": [
    "# We Trained, Now What?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cc5b3a0-a1f4-4214-aa1a-1783a6787656",
   "metadata": {},
   "source": [
    "## First, Let's Test\n",
    "1. Hit ESC-00 (reset the kernel), then evaluate performance by hand using the script below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a719f1a2-1270-4cd3-a6fd-5c0470d96b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import FullyShardedDataParallelPlugin, Accelerator\n",
    "from torch.distributed.fsdp.fully_sharded_data_parallel import FullOptimStateDictConfig, FullStateDictConfig\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
    "from datasets import load_dataset\n",
    "from peft import PeftModel\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "import re\n",
    "# !jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a1ef82b9-eb44-4668-bd98-82ca31475d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "llama_og_path = \"./models/llama-7B-huggingface\"\n",
    "llama_token_path = \"./models/llama-7B-huggingface\"\n",
    "dataset = \"./datasets/grammar_dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e3b3839-7707-401e-8046-f303101c79bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "checkpoint_path = \"llama2-grammar/checkpoint-300\" # TODO: Put Checkpoint path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27162a17-14b1-45d8-9c2a-d000639b993d",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset  = load_dataset(dataset, split='validation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "829a146a-73fb-4c36-81d7-c9c345b88eee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You set `add_prefix_space`. The tokenizer needs to be converted from the slow tokenizers\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    llama_token_path)\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "bos = tokenizer.bos_token\n",
    "eos = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5150a5f6-4961-49d2-949f-734c4db47fdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67626f945c134f9fb588f3cd8463b221",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load Model again\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_og_path, \n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "20d928e0-d515-489a-b5f4-9e18a1e8c5c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load LoRAs from checkpoint\n",
    "model = PeftModel.from_pretrained(model, checkpoint_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b59bd567-71de-4316-b250-df914b22a0a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/peft/tuners/lora/bnb.py:325: UserWarning: Merge lora module to 4-bit linear may get different generations due to rounding errors.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Merge LoRAs into model\n",
    "model = model.merge_and_unload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c347058-39e9-4eb3-a893-f1b61c436e8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/generation/utils.py:1510: UserWarning: You are calling .generate() with the `input_ids` being on a device type different than your model's device. `input_ids` is on cpu, whereas the model is on cuda. You may experience unexpected behaviors or slower generation. Please make sure that you have put `input_ids` to the correct device by calling for example input_ids = input_ids.to('cuda') before running `.generate()`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: Forexample, My cousin is 12years old. CORRECT: 4. For example, my cousin is twelve years old.\n",
      "The word “forexample” should be replaced with the phrase “for instance.” This is because it’s not a real word; it’s just an abbreviation for “for example,” which means that you can use either one in place of the other.\n",
      "What does forexample mean?\n"
     ]
    }
   ],
   "source": [
    "# Change i to change the input prompt. Evaluate the\n",
    "# outputs using your own judgement. Adjust hyperparameters above for training \n",
    "# or prompt to improve your response, and change max_new_tokens as appropriate\n",
    "\n",
    "i = 25\n",
    "max_new_tokens=75\n",
    "\n",
    "eval_prompt = f\"{bos}You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues,\" + \\\n",
    "        f\" the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {test_dataset[i]['input']} CORRECT: \" \n",
    "\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\")\n",
    "\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    output = tokenizer.decode(model.generate(**model_input, max_new_tokens=max_new_tokens, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef7b6a0-8264-4da7-881d-0271cbcbe04a",
   "metadata": {},
   "source": [
    "# Exporting The Model \n",
    "We SHOULD  be happy with the performance of the previous model, and want to move it to our local computer (For me, this is a MacBook Pro). Bits and Bytes (QLoRA lib) doesn't provide a way to export our weights back to float32 in a convienient way. So, we will have to write it ourselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ea19ae3c-516a-434d-a44b-23cfbaf19e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ab17a48d23a470dba3185ab7f97ebc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load original weights to make sure we get the right interface. This time we don't quantize\n",
    "\n",
    "target_model = AutoModelForCausalLM.from_pretrained(\n",
    "    llama_og_path, \n",
    "    device_map=\"cpu\" # Can't fit it all on the GPU. I'm using 32 gigs of ram\n",
    ")\n",
    "\n",
    "\n",
    "target_model = target_model.to(torch.bfloat16).to('cuda:0') # Now that we've switched to BFloat16, we can move to GPU as \n",
    "                                                            # it uses half the room\n",
    "\n",
    "\n",
    "target_state_dict = target_model.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9afa238f-6773-4013-814e-51fc98785dd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bitsandbytes import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac612b1-e4ef-4471-b1de-98e1e617e394",
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model.state_dict()\n",
    "# list(state_dict.keys())[:60]  # Commented out to truncate output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b4e5ccb-df09-464d-939f-7a3293d5eae6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_quant(name, sd) -> bool:\n",
    "    return name + '.nested_absmax' in sd.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5ecd31d-6107-4063-b72a-7629933795a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_layers = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63dce4cd-dc99-479a-87be-0932f4268665",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code de-quantizes weights in trained model, and sets the weights in the model in BFloat16 format to use the trained weights\n",
    "for layer in range(num_layers):\n",
    "    weights = [\n",
    "        # Can probably be done way cleaner, feel free to put in a pull request\n",
    "        (model.model.layers[layer].self_attn.q_proj.weight.quant_state, \n",
    "         f'model.layers.{layer}.self_attn.q_proj.weight',\n",
    "         f'model.layers.{layer}.self_attn.q_proj.weight'\n",
    "        ),\n",
    "        (model.model.layers[layer].self_attn.k_proj.weight.quant_state, \n",
    "         f'model.layers.{layer}.self_attn.k_proj.weight',\n",
    "         f'model.layers.{layer}.self_attn.k_proj.weight',\n",
    "        ),\n",
    "        (model.model.layers[layer].self_attn.v_proj.weight.quant_state, \n",
    "         f'model.layers.{layer}.self_attn.v_proj.weight',\n",
    "         f'model.layers.{layer}.self_attn.v_proj.weight',\n",
    "        ),\n",
    "        (model.model.layers[layer].self_attn.o_proj.weight.quant_state, \n",
    "         f'model.layers.{layer}.self_attn.o_proj.weight',\n",
    "         f'model.layers.{layer}.self_attn.o_proj.weight',\n",
    "        ),\n",
    "        (model.model.layers[layer].mlp.gate_proj.weight.quant_state, \n",
    "         f'model.layers.{layer}.mlp.gate_proj.weight',\n",
    "         f'model.layers.{layer}.mlp.gate_proj.weight'\n",
    "        ),\n",
    "        (model.model.layers[layer].mlp.up_proj.weight.quant_state,   \n",
    "         f'model.layers.{layer}.mlp.up_proj.weight',\n",
    "         f'model.layers.{layer}.mlp.up_proj.weight'\n",
    "        ),\n",
    "        (model.model.layers[layer].mlp.down_proj.weight.quant_state, \n",
    "         f'model.layers.{layer}.mlp.down_proj.weight',\n",
    "         f'model.layers.{layer}.mlp.down_proj.weight'\n",
    "        ),\n",
    "        (None,\n",
    "         f'model.layers.{layer}.input_layernorm.weight',\n",
    "         f'model.layers.{layer}.input_layernorm.weight'\n",
    "        ),\n",
    "        (None, \n",
    "         f'model.layers.{layer}.post_attention_layernorm.weight',\n",
    "         f'model.layers.{layer}.post_attention_layernorm.weight'\n",
    "        ),\n",
    "    ]\n",
    "    for q_state, key, target_path in weights:\n",
    "        if is_quant(key, state_dict):\n",
    "            F.dequantize_nf4(state_dict[key], q_state, out=target_state_dict[target_path])\n",
    "            torch.cuda.synchronize()\n",
    "            print(f\"Merged {target_path}\")\n",
    "        else:\n",
    "            target_state_dict[target_path] = state_dict[key].clone()\n",
    "            print(f\"Copied {target_path}\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "638066b2-6b98-4507-9d56-6c812a8d6a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we have to merge our special cases\n",
    "target_state_dict['model.embed_tokens.weight'] = state_dict['model.embed_tokens.weight'].clone()\n",
    "target_state_dict['model.norm.weight'] = state_dict['model.norm.weight'].clone()\n",
    "target_state_dict['lm_head.weight'] = state_dict['lm_head.weight'].clone()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0e0c43c3-ef98-494d-81e5-02a96061e760",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I'm not sure if this actually does anything due to the way torch tensor storage works under the hood. But let's be safe\n",
    "target_model.load_state_dict(target_state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9ecd497-34e6-4d51-bf5a-625d6f80ffd9",
   "metadata": {},
   "source": [
    "### Merge Sucessful!\n",
    "Before we get too excited, let's make sure that our model behaves as expected. You would not believe how many times I got the merge process wrong. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "dce131ee-7bb9-406d-8c02-623456f14f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues, the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: So, if i have alot of information about this subject, i will taulk too much with knowledge but if i have general information for this subject, i will talk about this subjec with my limited knowlege and this case may be make me shame like when my brother asked me about some thing but i have not alot of information about this thing. CORRECT: 1) If I have a lot of information about this subject, I will talk too much with knowledge; however, if I only have general information on this topic, I will speak about it with my limited knowledge and this might cause me to feel ashamed as when my brother asks me about something that I do not know very well.\n",
      "I'm sorry, but I\n"
     ]
    }
   ],
   "source": [
    "# Change i to change the input prompt. Evaluate the\n",
    "# outputs using your own judgement. Adjust hyperparameters above for training \n",
    "# or prompt to improve your response, and change max_new_tokens as appropriate\n",
    "\n",
    "i = 50\n",
    "max_new_tokens=75\n",
    "\n",
    "eval_prompt = f\"{bos}You will see two sentences. The first is marked INCORRECT and has a plethora of spelling and grammatical issues,\" + \\\n",
    "        f\" the second is marked CORRECT and shows the fixed version of the prior sentence. INCORRECT: {test_dataset[i]['input']} CORRECT: \" \n",
    "\n",
    "\n",
    "model_input = tokenizer(eval_prompt, return_tensors=\"pt\").to('cuda:0')\n",
    "\n",
    "target_model.eval()\n",
    "with torch.no_grad():\n",
    "    output = tokenizer.decode(target_model.generate(**model_input, max_new_tokens=max_new_tokens, repetition_penalty=1.15)[0], skip_special_tokens=True)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69d4d13c-f0b7-4078-9354-2662dcbe6b65",
   "metadata": {},
   "source": [
    "### Looks Good!\n",
    "Now we can download the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d1eeb77d-63d7-4ed9-b19a-9556eb537f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_model = target_model.to('cpu').to('float32') # Float 32 is most precise, so gotta convert back first\n",
    "\n",
    "target_model.save_pretrained(\"models/model-final\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89db3efe-04c0-4d22-a5a1-e30a4495ab62",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Exporting to Local"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b8057a-7a25-4cc9-8937-799086b16a03",
   "metadata": {},
   "source": [
    "\n",
    "We download our weights by taring them up for a faster download (11gb vs 22gb)\n",
    "```console\n",
    "cd models\n",
    "cp llama-7B-huggingface/special_tokens_map.json model-final\n",
    "cp llama-7B-huggingface/tokenizer* model-final\n",
    "cd ..\n",
    "tar -cvf final-model.tar.gz models/model-final\n",
    "```\n",
    "\n",
    "Now on your local instance, in a clone of Llama.cpp (Use my modifications here, under \"examples/llamacheck\" (https://github.com/Ferruolo/llama.cpp).\n",
    "```\n",
    "# In base of llama.cpp directory\n",
    "scp -i myBrevInstanceName:MyFirstLLM-BlogPost/final-model.tar.gz ./models/\n",
    "tar -xvf ./models/final-model.tar.gz\n",
    "python convert-hf-to-gguf.py ./models/final-model --outfile models/llamacheck-dequant.gguf\n",
    "./quantize models/llamacheck-dequant.gguf models/llamacheck.gguf Q4_0\n",
    "make llamacheck\n",
    "./llamacheck ./models/llamacheck.gguf\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdfd5b4e-4468-42cb-b54d-e04b8b37796a",
   "metadata": {},
   "source": [
    "# We're Done! A Brief Reflection\n",
    "\n",
    "Being honest, my experience was not as simple as this blog post suggests. Even though I adapted most of my code directly from the notebooks I actually used, my initial process took a lot of twists and turns. I misinterpreted docs several times and struggled to export my weights. Don't be dismayed if things are difficult for you, Machine Learning is hard! I hope that by compiling everything I did into one big notebook, I have made your life significantly easier. I was suprised by how easy finetuning was once I had figured everything out, but was dismayed at how difficult doing practical things with HuggingFace, Bits and Bytes library (QLORA), and llama.cpp were. I found myself very confused, and had to spend a lot of time reading codebases to figure everything out.\n",
    "\n",
    "\n",
    "I would like to thank the Brev team for giving me the opportunity to write this blog for them, and helping me along the way. If you have any followup questions, or would like to reach out to me for another reason, please contact my email at andrew.ferruolo@gmail.com"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
